{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series classification - MLP\n",
    "<a href=\"https://colab.research.google.com/github/jarusgnuj/ioctm358/blob/master/notebooks/time_series_classification/2_TSC_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf # Fast numerical computation for machine learning, computations on GPU or CPU\n",
    "import tensorflow.keras as keras # High-level interface to TensorFlow, making it easier to create neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# General settings and variables\n",
    "sns.set_style('whitegrid')\n",
    "model_palette = ['rebeccapurple', 'mediumspringgreen']\n",
    "\n",
    "class_names = ['cement', 'carpet']\n",
    "class_colors = ['darkorange', 'steelblue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Functions\n",
    "Some functions, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    ''' Load the data from a file in a GitHub repo '''\n",
    "    url_root = 'https://raw.githubusercontent.com/jarusgnuj/ai-ml-wksh/master/data/UCR_TSC_archive/SonyAIBORobotSurface1_IoC'\n",
    "    url = url_root+'/'+filename\n",
    "    robot_df = pd.read_csv(url, sep='\\t', header=None)\n",
    "    print('Loaded from', url)\n",
    "    robot_data = robot_df.values\n",
    "    print('The shape of robot_data is', robot_data.shape)\n",
    "    return robot_data\n",
    "\n",
    "\n",
    "def preprocess_data(robot_data):\n",
    "    ''' Split the data into data samples and labels. Convert class labels in to 0 and 1 '''\n",
    "    labels = robot_data[:,0]\n",
    "    data_samples = robot_data[:,1:]\n",
    "    print('The shape of the data matrix is', data_samples.shape)\n",
    "    print('The shape of the labels vector is', labels.shape)\n",
    "\n",
    "    # Change from classes 1 and 2 to classes 0 and 1, for convenience of use with Keras\n",
    "    labels = labels - 1\n",
    "    labels = labels.astype(int)\n",
    "    print('Number of samples of class 0', (labels == 0).sum())\n",
    "    print('Number of samples of class 1', (labels == 1).sum())\n",
    "\n",
    "    return data_samples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Load the development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SonyAIBORobotSurface1_IoC_DEV.txt'\n",
    "robot_data = load_data(filename)\n",
    "data, labels = preprocess_data(robot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Split the development dataset into training and test datasets\n",
    "In the UEA & UCR Time Series Classification Repository 20 samples are used for training and 601 are used for testing.\n",
    "\n",
    "\n",
    "We've already discarded some samples to provide a balanced dataset and we have already reserved 100 samples for our final test set. This leaves us with 444 data samples in this development phase.\n",
    "\n",
    "\n",
    "How much data do you want to reserve for testing in this development phase? If you test on only 10 samples, would you be confident of the result?\n",
    "\n",
    "\n",
    "![The development dataset is split into two, unequal sets](images/train_test_split.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 424 ### CHANGE PARAMETER HERE ###\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    data, labels, test_size=test_size, random_state=21, stratify=labels)\n",
    "\n",
    "print('The shape of train_data is', data_train.shape)\n",
    "print('The shape of test_data is', data_test.shape)\n",
    "print('Training data:')\n",
    "print('Number of samples of class 0', (labels_train == 0).sum())\n",
    "print('Number of samples of class 1', (labels_train == 1).sum())\n",
    "print('Test data:')\n",
    "print('Number of samples of class 0', (labels_test == 0).sum())\n",
    "print('Number of samples of class 1', (labels_test == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 MLP\n",
    "Create a multilayer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the input vector\n",
    "input_dim = data_train.shape[1]\n",
    "print('input_dim:', input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(print_summary=False):\n",
    "    ''' Return a model with randomly initialised weights '''\n",
    "    model = Sequential([\n",
    "        Dense(16, input_dim=input_dim, activation='relu', name='Layer1'), \n",
    "        Dense(8, activation='relu', name='Layer2'), \n",
    "        Dense(1, activation='sigmoid', name='OutputLayer')\n",
    "    ])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam() \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 MLP neurons and connections\n",
    "![Visualising the MLP model. 70 inputs. 2 hidden layers - 16 neurons in the first layer, 8 in the second. One neuron in the output layer](images/mlp.png \"MLP architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Test the untrained MLP\n",
    "Let's test the model before we train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(data_test, labels_test, batch_size=5)\n",
    "print('Pre-training, validation accuracy is', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Train the MLP\n",
    "batch_size - how many data samples to input to the MLP in one go.\n",
    "\n",
    "\n",
    "epochs - how many times the training process will iterate through the entire training set.\n",
    "\n",
    "Detail\n",
    "+ For the first step of training, a batch of samples from the training dataset are input the to MLP. \n",
    "  + The number of samples in the batch is set by the batch_size parameter.\n",
    "+ The loss function's value is calculated and the MLP's weights are updated in a way that is expected to reduce the loss value in the next step. \n",
    "+ In the second step, the next batch of samples is input to the MLP. The loss value is calculated and the weights are updated.\n",
    "+ These batch calculations and updates continue until all of the samples in the training dataset have been used. \n",
    "  + This completes one epoch.\n",
    "+ The subsequent epochs similarly cycle through the training dataset batch by batch.\n",
    "+ Training stops when the number of epochs, a parameter set by the user below, has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 ### CHANGE PARAMETER HERE ### \n",
    "epochs = 50 ### CHANGE PARAMETER HERE ###\n",
    "\n",
    "model = build_model() # This re-initialises the model with random weights each time before we train it.\n",
    "\n",
    "# Train\n",
    "start = time.time()\n",
    "hist = model.fit(data_train, labels_train, batch_size=batch_size, epochs=epochs, \n",
    "                 validation_data=(data_test, labels_test), verbose=1)\n",
    "end = time.time()\n",
    "log = pd.DataFrame(hist.history) \n",
    "print('Training complete in', round(end-start), 'seconds')\n",
    "\n",
    "# Use the trained model to classify the test dataset.\n",
    "result = model.evaluate(data_test, labels_test, batch_size=batch_size)\n",
    "print('Validation loss:', result[0])\n",
    "print('Validation accuracy:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Exercise 2a : Variability\n",
    "Rerun the model training cell a few times. Each time, note down the validation accuracy (printed at the end of the output above). How much does this vary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User comments\n",
    "Enter your result for each run below -\n",
    "+ Run 1: validation accuracy = \n",
    "+ Run 2: validation accuracy =\n",
    "+ Run 3: validation accuracy =\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Model training progress\n",
    "We can plot model performance as training progresses using the training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model training produced a log of how the training progressed\n",
    "print('The last five rows in the log are')\n",
    "log.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log's loss data.\n",
    "ax = log[['loss', 'val_loss']].plot(title='Loss function during training', color=model_palette)\n",
    "ax.set_xlabel(\"Model training epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend([\"training\", \"validation\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Plot the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log's accuracy data.\n",
    "ax = log[['acc', 'val_acc']].plot(title='Accuracy during training', color=model_palette)\n",
    "ax.set_xlabel(\"Model training epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend([\"training\", \"validation\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Exercise 2b : Training choices\n",
    "The cells above include some numbers that you can change, indicated with ### CHANGE PARAMETER HERE ###. Go ahead and change these and rerun the cells in order to answer the following questions.\n",
    "+ How much of the development dataset do you think you need use for training, how much for testing?\n",
    "+ How many training epochs are needed?\n",
    "+ What batch size works well?\n",
    "\n",
    "Note your choices below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User comments\n",
    "Enter your own notes below.\n",
    "Good choices of parameters are:-\n",
    "+ test_size \n",
    "+ epochs\n",
    "+ batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Make predictions using the trained MLP\n",
    "This trained model can now be used to classify data samples. These could be samples from our test set, our final test set or completely new samples.\n",
    "\n",
    "\n",
    "Select a single test sample and ask the model to 'predict' its class. The model gives you the probability that this sample belongs to class 1 (carpet). \n",
    "\n",
    "Typically, if the probability is above 0.5, we classify the sample as belonging to class 1. If the probability is 0.5 or lower, we classify the sample as belonging to class 0.\n",
    "\n",
    "+ What class does the model assign to this sample?\n",
    "+ What is its true class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 5 ### CHANGE PARAMETER HERE ###\n",
    "data_sample = data_test[sample_num]\n",
    "data_sample = np.array( [data_sample,] ) # Convert the data sample into the shape expected by the MLP\n",
    "probability = model.predict(data_sample)\n",
    "print('Model: probability of belonging to class 1:', probability[0][0])\n",
    "print('Predicted class:\\t', (np.round(probability)[0][0].astype(int))) # \\t inserts a tab space into the text\n",
    "print('True class:\\t\\t', labels_test[sample_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Batch predictions\n",
    "You can ask the model to make predictions on the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_probability = model.predict_on_batch(data_test)\n",
    "labels_predicted_class = np.round(labels_probability).flatten()\n",
    "print('Some of the test results:')\n",
    "print('True', labels_test[:23])\n",
    "print('Pred', labels_predicted_class[:23].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Exercise 2c : Insight into model performance\n",
    "+ Identify a test data sample that the model classifies incorrectly.\n",
    "+ Compare this sample to training data in the same class.\n",
    "+ How do they compare?\n",
    "  + What might this tell you about either the data, the training or about the model's ability?\n",
    "  \n",
    "  \n",
    "Repeat your analysis with a few more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = 2 ### CHANGE PARAMETER HERE ###\n",
    "print('Test sample', test_sample, 'true class', str(labels_test[test_sample]), class_names[labels_test[test_sample]])  \n",
    "true_class = labels_test[test_sample]\n",
    "\n",
    "# Plot training data samples that are in the same class\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(10):\n",
    "    if labels_train[i] == true_class:\n",
    "        plt.plot(data_train[i], color=class_colors[labels_train[i]])\n",
    "        print('Training sample', i, 'class', str(labels_train[i]), class_names[labels_train[i]])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Walking on  '+class_names[true_class])\n",
    "ax.set_ylabel('Standardised x-axis accelerometer data')\n",
    "ax.set_xlabel('Data point number')\n",
    "\n",
    "# Plot the test data sample\n",
    "plt.plot(data_test[test_sample], color='darkred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Optional : Model prediction probabilities\n",
    "Find some test data samples where the model made an incorrect classification and predicted a high probability that the sample belongs to that class. E.g. a sample of true class 0 where the model predicted a probability of 0.95 that the sample belongs to class 1.\n",
    "\n",
    "\n",
    "Continue exercise 2c focusing on these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_probability.shape[0])\n",
    "x = np.arange(labels_probability.shape[0])\n",
    "class_cmap = matplotlib.colors.ListedColormap(class_colors)\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(np.arange(labels_probability.shape[0]), labels_probability, linestyle='None', marker='x', \n",
    "            c=labels_test, cmap=class_cmap)\n",
    "plt.title('Orange: true class 0 (cement)\\nBlue: true class 1 (carpet)')\n",
    "ax.set_xlabel('Test sample number')\n",
    "ax.set_ylabel('Model: probability of belonging to class 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
