{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series classification - Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf # Fast numerical computation for machine learning, computations on GPU or CPU\n",
    "import tensorflow.keras as keras # High-level interface to TensorFlow, making it easier to create neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# General settings and variables\n",
    "sns.set_style('whitegrid')\n",
    "model_palette = ['rebeccapurple', 'mediumspringgreen']\n",
    "\n",
    "class_names = ['cement', 'carpet']\n",
    "class_colors = ['darkorange', 'steelblue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Functions\n",
    "Some functions, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filename):\n",
    "    ''' Load the dataset file and return the data and labels '''\n",
    "    # Load the dataset\n",
    "    url_root = 'https://raw.githubusercontent.com/jarusgnuj/ai-ml-wksh/master/data/UCR_TSC_archive/SonyAIBORobotSurface1_IoC'\n",
    "    url = url_root+'/'+filename\n",
    "    robot_df = pd.read_csv(url, sep='\\t', header=None)\n",
    "    print('Loaded from', url)\n",
    "    robot_data = robot_df.values\n",
    "    \n",
    "    # Separate out the data and labels\n",
    "    labels = robot_data[:,0].astype(int)\n",
    "    data_samples = robot_data[:,1:]\n",
    "    \n",
    "    # Print information\n",
    "    print('The shape of the data matrix is', data_samples.shape)\n",
    "    print('Number of samples of class 0', (labels == 0).sum())\n",
    "    print('Number of samples of class 1', (labels == 1).sum())\n",
    "\n",
    "    return data_samples, labels\n",
    "    \n",
    "    \n",
    "def plot_comparison(data_train, labels_train, data_test, labels_test, test_sample):\n",
    "    ''' Plot the given test sample alongside a few training samples of the same class '''\n",
    "    # Determine the true class of the given sample\n",
    "    print('Test sample', test_sample, 'true class', str(labels_test[test_sample]), class_names[labels_test[test_sample]])  \n",
    "    true_class = labels_test[test_sample]\n",
    "\n",
    "    # Plot data samples that are in the same class\n",
    "    fig, ax = plt.subplots()\n",
    "    count = 0\n",
    "    for i in range(100):\n",
    "        if labels_train[i] == true_class:\n",
    "            plt.plot(data_train[i], color=class_colors[labels_train[i]])\n",
    "            print('Training sample', i, 'class', str(labels_train[i]), class_names[labels_train[i]])\n",
    "            count = count + 1\n",
    "            if count > 4:\n",
    "                break\n",
    "    plt.ylim([-3.5, 3.5])\n",
    "    plt.title('Walking on  '+class_names[true_class])\n",
    "    ax.set_ylabel('Accelerometer data')\n",
    "    ax.set_xlabel('Data point number')\n",
    "\n",
    "    # Plot the test data sample\n",
    "    plt.plot(data_test[test_sample], color='darkred')\n",
    "    \n",
    "    \n",
    "def plot_loss(log):\n",
    "    ''' Plot the loss recorded in the log during model training '''\n",
    "    ax = log[['loss', 'val_loss']].plot(title='Loss function during training', color=model_palette)\n",
    "    ax.set_xlabel(\"Model training epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend([\"training\", \"validation\"]);\n",
    "    \n",
    "    \n",
    "def plot_accuracy(log):\n",
    "    ''' Plot the accuracy recorded in the log during model training '''\n",
    "    ax = log[['acc', 'val_acc']].plot(title='Accuracy during training', color=model_palette)\n",
    "    ax.set_xlabel(\"Model training epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend([\"training\", \"validation\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Load the development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SonyAIBORobotSurface1_IoC_DEV.txt'\n",
    "data, labels = load_data_and_labels(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Split the development dataset into training and test datasets\n",
    "We have already reserved 100 samples for our final test set. This leaves us with 444 data samples in this development phase.\n",
    "\n",
    "How much data do you want to use for training the model and how much for testing in this development phase? \n",
    "\n",
    "\n",
    "![The development dataset is split into two, unequal sets](images/train_test_split.png \"Title\")\n",
    "\n",
    "Considerations -\n",
    "+ If you test on only 10 samples, would you be confident of the result?\n",
    "+ How long does it take to train the model if the training set contains 20 samples, as compared to 400 samples?\n",
    "+ Does model accuracy continue to improve as training set size is increased?\n",
    "\n",
    "Rough guide -\n",
    "+ If the dataset contains fewer than 1 million samples, then a typical training:test:final test split would be 80%:10%:10% or 60%:20%:20% \n",
    "+ If the dataset is larger, over a million samples, then a practitioner might choose to use around 10,000 samples for test, 10,000 samples for final test and the remainder for training. Or they might split the data 99.5%:0.25%:0.25%\n",
    "\n",
    "Note -\n",
    "+ \"Test dataset\" and the \"validation dataset\" refer to the same dataset. We train the model using the training dataset and use the test dataset to measure validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 100 ### CHANGE PARAMETER HERE ###\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    data, labels, test_size=test_size, random_state=21, stratify=labels)\n",
    "\n",
    "print('The shape of train_data is', data_train.shape)\n",
    "print('The shape of test_data is', data_test.shape)\n",
    "print('Training data:')\n",
    "print('Number of samples of class 0', (labels_train == 0).sum())\n",
    "print('Number of samples of class 1', (labels_train == 1).sum())\n",
    "print('Test data:')\n",
    "print('Number of samples of class 0', (labels_test == 0).sum())\n",
    "print('Number of samples of class 1', (labels_test == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Multilayer Perceptron\n",
    "Create a multilayer perceptron (MLP) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 MLP neurons and connections\n",
    "![Visualising the MLP model. 70 inputs. 2 hidden layers - 16 neurons in the first layer, 8 in the second. One neuron in the output layer](images/mlp.png \"MLP architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the input vector\n",
    "input_dim = data_train.shape[1]\n",
    "\n",
    "\n",
    "def build_model(print_summary=False):\n",
    "    ''' Return a model with randomly initialised weights '''\n",
    "    model = Sequential([\n",
    "        Dense(8, input_dim=input_dim, activation='relu', name='Layer1'), \n",
    "        Dense(4, activation='relu', name='Layer2'), \n",
    "        Dense(1, activation='sigmoid', name='OutputLayer')\n",
    "    ])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam() \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Test the untrained MLP\n",
    "Let's test the model before we train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(data_test, labels_test, batch_size=5)\n",
    "print('Pre-training, validation accuracy is', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Train the MLP\n",
    "batch_size - how many data samples to input to the MLP in one go.\n",
    "\n",
    "\n",
    "epochs - how many times the training process will iterate through the entire training set.\n",
    "\n",
    "Detail\n",
    "+ For the first step of training, a batch of samples from the training dataset are input the to MLP. \n",
    "  + The number of samples in the batch is set by the batch_size parameter.\n",
    "+ The loss function's value is calculated and the MLP's weights are updated in a way that is expected to reduce the loss value in the next step. \n",
    "+ In the second step, the next batch of samples is input to the MLP. The loss value is calculated and the weights are updated.\n",
    "+ These batch calculations and updates continue until all of the samples in the training dataset have been used. \n",
    "  + This completes one epoch.\n",
    "+ The subsequent epochs similarly cycle through the training dataset batch by batch.\n",
    "+ Training stops when the number of epochs, a parameter set by the user below, has been reached.\n",
    "\n",
    "Batch size is typically set to a power of two (2, 4, 8, 16, ...) to best utilse the memory size of a CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 ### CHANGE PARAMETER HERE ### \n",
    "epochs = 20 ### CHANGE PARAMETER HERE ###\n",
    "\n",
    "model = build_model() # This re-initialises the model with random weights each time before we train it.\n",
    "\n",
    "# Train\n",
    "start = time.time()\n",
    "hist = model.fit(data_train, labels_train, batch_size=batch_size, epochs=epochs, \n",
    "                 validation_data=(data_test, labels_test), verbose=1)\n",
    "end = time.time()\n",
    "log = pd.DataFrame(hist.history) \n",
    "print('Training complete in', round(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to classify the test dataset.\n",
    "result = model.evaluate(data_test, labels_test, batch_size=batch_size)\n",
    "print('Validation accuracy:\\t', result[1])\n",
    "print('Validation loss:\\t', result[0])\n",
    "print('test_size:\\t', test_size)\n",
    "print('batch_size:\\t', batch_size)\n",
    "print('epochs:\\t\\t', epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Exercise 2a : Variability\n",
    "Rerun the model training and the model evaluation in the two cells above. Each time, note down (in the cell below) the validation accuracy. How much does it vary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User comments\n",
    "Enter your result for each run below -\n",
    "+ Run 1: validation accuracy = \n",
    "+ Run 2: validation accuracy =\n",
    "+ Run 3: validation accuracy =\n",
    "+ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Model training progress\n",
    "We can plot model performance as training progresses using the training log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training log's loss data.\n",
    "plot_loss(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Plot the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training log's accuracy data.\n",
    "plot_accuracy(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Exercise 2b : Training choices\n",
    "The cells above include some numbers that you can change -\n",
    "+ test_size \n",
    "+ epochs\n",
    "+ batch_size\n",
    "\n",
    "They are indicated with ### CHANGE PARAMETER HERE ###. Go ahead and change these and rerun the cells in order to answer the following questions.\n",
    "+ How much of the development dataset do you think you need use for training, how much for testing?\n",
    "+ How many training epochs are needed?\n",
    "+ What batch size works well?\n",
    "\n",
    "Note your choices below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User comments\n",
    "Enter your own notes below.\n",
    "Good choices of parameters are:-\n",
    "+ test_size \n",
    "+ epochs\n",
    "+ batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Make predictions using the trained MLP\n",
    "This trained model can now be used to classify data samples. These could be samples from our test set, our final test set or completely new samples.\n",
    "\n",
    "In machine learning, the class that is output by the model is called a \"prediction\". A data sample is input to the model and the model returns its \"predicted class\". If the model is correct, this predicted class will match data sample's \"true\" class.\n",
    "\n",
    "\n",
    "In the cell below we will select a single test sample and ask the model to 'predict' its class. The model gives you the probability that this sample belongs to class 1 (carpet). \n",
    "\n",
    "Typically, if the probability is above 0.5, we classify the sample as belonging to class 1. If the probability is 0.5 or lower, we classify the sample as belonging to class 0.\n",
    "\n",
    "+ What class does the model assign to this sample?\n",
    "+ What is its true class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 5 ### CHANGE PARAMETER HERE ###\n",
    "data_sample = data_test[sample_num]\n",
    "data_sample = np.array( [data_sample,] ) # Convert the data sample into the shape expected by the MLP\n",
    "probability = model.predict(data_sample)\n",
    "print('Model: probability of belonging to class 1:', probability[0][0])\n",
    "print('Predicted class:\\t', (np.round(probability)[0][0].astype(int))) # \\t inserts a tab space into the text\n",
    "print('True class:\\t\\t', labels_test[sample_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Optional : Batch predictions\n",
    "You can ask the model to make predictions on set of data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_probability = model.predict_on_batch(data_test)\n",
    "labels_predicted_class = np.round(labels_probability).flatten()\n",
    "print('Some of the test results:')\n",
    "print('True', labels_test[:23])\n",
    "print('Pred', labels_predicted_class[:23].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Optional : Exercise 2c : Insight into model performance\n",
    "+ Identify a test data sample that the model classifies incorrectly.\n",
    "+ Compare this sample to training data in the same class.\n",
    "+ How do they compare?\n",
    "  + What might this tell you about either the data, the training or about the model's ability?\n",
    "  \n",
    "  \n",
    "Repeat your analysis with a few more samples.\n",
    "\n",
    "Continue on to the next optional section if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = 6 ### CHANGE PARAMETER HERE ###\n",
    "plot_comparison(data_train, labels_train, data_test, labels_test, test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Optional : Model prediction probabilities\n",
    "Find some test data samples where the model made an incorrect classification and predicted a high probability that the sample belongs to that class. E.g. a sample of true class 0 where the model predicted a probability of 0.95 that the sample belongs to class 1.\n",
    "\n",
    "\n",
    "Continue exercise 2c focusing on these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(labels_probability.shape[0])\n",
    "class_cmap = matplotlib.colors.ListedColormap(class_colors)\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(np.arange(labels_probability.shape[0]), labels_probability, linestyle='None', marker='x', \n",
    "            c=labels_test, cmap=class_cmap)\n",
    "plt.title('Orange: true class 0 (cement)\\nBlue: true class 1 (carpet)')\n",
    "ax.set_xlabel('Test sample number')\n",
    "ax.set_ylabel('Model: probability of belonging to class 1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
