{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful tools for TSC\n",
    "<a href=\"https://colab.research.google.com/github/jarusgnuj/ioctm358/blob/master/notebooks/time_series_classification/Appendix_1_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Split the dataset into development and final test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.utils\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    ''' Load the data from a file in a GitHub repo '''\n",
    "    url_root = 'https://raw.githubusercontent.com/jarusgnuj/ai-ml-wksh/master/data/UCR_TSC_archive/SonyAIBORobotSurface1_IoC'\n",
    "    url = url_root+'/'+filename\n",
    "    robot_df = pd.read_csv(url, sep='\\t', header=None)\n",
    "    print('Loaded from', url)\n",
    "    robot_data = robot_df.values\n",
    "    print('The shape of robot_data is', robot_data.shape)\n",
    "    return robot_data\n",
    "\n",
    "\n",
    "robot_data = load_data('SonyAIBORobotSurface1_IoC_ALL.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = robot_data[:,0]\n",
    "print('Number of samples of class 1', (y_data == 1.0).sum())\n",
    "print('Number of samples of class 2', (y_data == 2.0).sum())\n",
    "y_df = pd.DataFrame(robot_data[:,0])\n",
    "y_df[0].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data. \n",
    "# If your data samples are in chronological order, this will mix them up.\n",
    "robot_data = sklearn.utils.shuffle(robot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_df = pd.DataFrame(robot_data)\n",
    "class1_df = robot_df[(robot_df[0]==1)]\n",
    "class2_df = robot_df[(robot_df[0]==2)]\n",
    "n1 = class1_df.count()[0]\n",
    "n2 = class2_df.count()[0]\n",
    "n = min(n1, n2)\n",
    "print('Selection set sizes:', n1, n2)\n",
    "print('Min selection set size:', n)\n",
    "class1_df = class1_df.iloc[:n]\n",
    "class2_df = class2_df.iloc[:n]\n",
    "balanced_df = pd.concat([class1_df, class2_df])\n",
    "balanced_df = sklearn.utils.shuffle(balanced_df)\n",
    "balanced_df[0].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'SonyAIBORobotSurface1_IoC'\n",
    "data_filename = data_name+'_BALANCED.txt'\n",
    "if do_save:   \n",
    "    np.savetxt(data_filename, balanced_df.to_numpy(), fmt='%8e', delimiter='\\t')\n",
    "    print('Data saved to file', data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload and check the balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = np.loadtxt(Path(data_filename))\n",
    "print('The shape of balanced_data is', balanced_data.shape)\n",
    "y_data = balanced_data[:,0]\n",
    "print('Number of samples of class 1', (y_data == 1.0).sum())\n",
    "print('Number of samples of class 2', (y_data == 2.0).sum())\n",
    "y_df = pd.DataFrame(y_data)\n",
    "y_df[0].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(balanced_data, test_size=100, random_state=21, stratify=balanced_data[:,0])\n",
    "print('The shape of train_data is', train_data.shape)\n",
    "print('The shape of test_data is', test_data.shape)\n",
    "print('Training data:')\n",
    "print('Number of samples of class 1', (train_data[:,0] == 1.0).sum())\n",
    "print('Number of samples of class 2', (train_data[:,0] == 2.0).sum())\n",
    "print('Test data:')\n",
    "print('Number of samples of class 1', (test_data[:,0] == 1.0).sum())\n",
    "print('Number of samples of class 2', (test_data[:,0] == 2.0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the development and final test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'SonyAIBORobotSurface1_IoC'\n",
    "if do_save:\n",
    "    data_filename = data_name+'_DEV.txt'\n",
    "    np.savetxt(data_filename, train_data, fmt='%8e', delimiter='\\t')\n",
    "    data_filename = data_name+'_FINAL_TEST.txt'\n",
    "    np.savetxt(data_filename, test_data, fmt='%8e', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardise the data\n",
    "We find that the robot data has already been standardised; the full dataset has a mean of zero and a standard deviation of one. Here we show how a dataset can be standardised. The StandardScalar transform function subtracts the mean of the training set (u) and divides by the standard deviation of the training set (s) and returns\n",
    "\n",
    "\n",
    "z = (data - u)/s\n",
    "\n",
    "\n",
    "Notice that we use the mean and standard deviation of the training set to standardise the test data. This avoids leaking information about the test set into our test data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mean = train_data.mean()\n",
    "train_data_std = train_data.std()\n",
    "print('train_data_mean', train_data_mean)\n",
    "print('train_data_std', train_data_std)\n",
    "\n",
    "do_standardise = False\n",
    "if do_standardise:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data) # (train_data - train_data_mean)/(train_data_std) \n",
    "    test_data = scaler.transform(test_data)   # (test_data - train_data_mean)/(train_data_std)\n",
    "    print('Standardisation done.')\n",
    "    print('train_data.mean()', train_data.mean())\n",
    "    print('train_data.std()', train_data.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed - GPU\n",
    "Using a GPU can speed up calculations. However, it can take longer to transfer the data to the GPU.\n",
    "\n",
    "You are more likely to see a speed-up if batch size is large. As you increase batch size, check that valuation accuracy does not deteriorate.\n",
    "\n",
    "To use a GPU in colab select Edit - Notebook settings and then set Hardware accelerator to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if you are using a GPU.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found a GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
