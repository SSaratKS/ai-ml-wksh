{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series classification - model development\n",
    "This notebook is a stripped down version of notebook 2, so you can focus on model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf # Fast numerical computation for machine learning, computations on GPU or CPU\n",
    "import tensorflow.keras as keras # High-level interface to TensorFlow, making it easier to create neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# General settings and variables\n",
    "sns.set_style('whitegrid')\n",
    "model_palette = ['rebeccapurple', 'mediumspringgreen']\n",
    "\n",
    "class_names = ['cement', 'carpet']\n",
    "class_colors = ['darkorange', 'steelblue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Functions\n",
    "Some functions, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filename):\n",
    "    ''' Load the dataset file and return the data and labels '''\n",
    "    # Load the dataset\n",
    "    url_root = 'https://raw.githubusercontent.com/jarusgnuj/ai-ml-wksh/master/data/UCR_TSC_archive/SonyAIBORobotSurface1_IoC'\n",
    "    url = url_root+'/'+filename\n",
    "    robot_df = pd.read_csv(url, sep='\\t', header=None)\n",
    "    print('Loaded from', url)\n",
    "    robot_data = robot_df.values\n",
    "    \n",
    "    # Separate out the data and labels\n",
    "    labels = robot_data[:,0].astype(int)\n",
    "    data_samples = robot_data[:,1:]\n",
    "    \n",
    "    # Print information\n",
    "    print('The shape of the data matrix is', data_samples.shape)\n",
    "    print('Number of samples of class 0', (labels == 0).sum())\n",
    "    print('Number of samples of class 1', (labels == 1).sum())\n",
    "\n",
    "    return data_samples, labels\n",
    "\n",
    "\n",
    "def plot_loss(log):\n",
    "    ''' Plot the loss recorded in the log during model training '''\n",
    "    ax = log[['loss', 'val_loss']].plot(title='Loss function during training', color=model_palette)\n",
    "    ax.set_xlabel(\"Model training epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend([\"training\", \"validation\"]);\n",
    "    \n",
    "    \n",
    "def plot_accuracy(log):\n",
    "    ''' Plot the accuracy recorded in the log during model training '''\n",
    "    ax = log[['acc', 'val_acc']].plot(title='Accuracy during training', color=model_palette)\n",
    "    ax.set_xlabel(\"Model training epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend([\"training\", \"validation\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Load the development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SonyAIBORobotSurface1_IoC_DEV.txt'\n",
    "data, labels = load_data_and_labels(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 100 ### CHANGE PARAMETER HERE ###\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    data, labels, test_size=test_size, random_state=21, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Exercise 3a: Model development\n",
    "In the build_model function you can change the number of nodes in each layer. You can also add and remove layers to change the number of layers. These are some of the model's \"hyperparameters\". \n",
    "\n",
    "Optional - use the dropout layer that is introduced at the end of this notebook if you wish.\n",
    "\n",
    "+ Set test_size, batch_size, epochs to the values you settled on in the previous notebook.\n",
    "+ Change model hyperparameters then build, train and evaluate the model.\n",
    "  + You may wish to re-examine your choice of test_size, batch_size and epochs periodically.\n",
    "+ What hyperparameter settings give consistently high validation accuracy?\n",
    "+ When you have settled on the best set of hyperparameters, re-run model training five times and calculate the mean mean average validation accuracy (see section 4.3.1).\n",
    "\n",
    "\n",
    "## Competition part I\n",
    "+ The best development model - highest mean validation accuracy over five training runs.\n",
    "+ Tie-breaker - the lowest sample standard deviation.\n",
    "\n",
    "## Competition part II \n",
    "This is in the next notebook.\n",
    "+ Best performing model - the highest accuracy when tested on the final test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Build model\n",
    "Create a multilayer perceptron (MLP) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the input vector\n",
    "input_dim = data_train.shape[1]\n",
    "\n",
    "\n",
    "def build_model(print_summary=False):\n",
    "    ''' Return a model with randomly initialised weights '''\n",
    "    ### CHANGE PARAMETERS HERE ###\n",
    "    # Change the number of nodes in each layer.\n",
    "    # Add or remove layers.\n",
    "    model = Sequential([\n",
    "        Dense(16, input_dim=input_dim, activation='relu', name='Layer1'), \n",
    "        Dense(8, activation='relu', name='Layer2'), \n",
    "        Dense(1, activation='sigmoid', name='OutputLayer')\n",
    "    ])\n",
    "    ### END OF CHANGE PARAMETERS ###\n",
    "\n",
    "    optimizer = keras.optimizers.Adam() \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(print_summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 ### CHANGE PARAMETER HERE ### \n",
    "epochs = 20 ### CHANGE PARAMETER HERE ###\n",
    "\n",
    "model = build_model() # This re-initialises the model with random weights each time before we train it.\n",
    "\n",
    "# Train\n",
    "start = time.time()\n",
    "hist = model.fit(data_train, labels_train, batch_size=batch_size, epochs=epochs, \n",
    "                 validation_data=(data_test, labels_test), verbose=1)\n",
    "end = time.time()\n",
    "log = pd.DataFrame(hist.history) \n",
    "print('Training complete in', round(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to classify the test dataset.\n",
    "result = model.evaluate(data_test, labels_test, batch_size=batch_size)\n",
    "print('Validation accuracy:\\t', result[1])\n",
    "print('Validation loss:\\t', result[0])\n",
    "print('test_size:\\t', test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Calculate the mean average validation accuracy\n",
    "When you have settled on the best set of hyperparameters, re-run the model training 5 times. Enter your validation accuracy results in the array below to calculate the mean and sample standard deviation. Sample standard deviation is a measure of how variable your results are; a lower value relates to lower variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_results = np.array([0.70, 0.80, 0.73, 0.88, 0.93]) ### CHANGE PARAMETERS HERE ###\n",
    "print('Mean average:', val_acc_results.mean())\n",
    "print('Sample standard deviation:', val_acc_results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Optional : dropout\n",
    "Dropout layers can improve a model's generalisation. A dropout layer randomly turns off some of the nodes during each training batch calculation. This can prevent a model from overfitting to the training data. Overfitting can result in high training accuracy but low validation accuracy. \n",
    "+ Does your model suffer from this?\n",
    "+ What happens if you increase the number nodes in the model does validation accuracy start to decrease, suggesting that overfitting is occurring?\n",
    "\n",
    "Try adding dropout layers to your model. An example of such a model is given in the notebook Appendix_4_dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Optional : k-fold cross validation\n",
    "k-fold cross validation is a method for repeated training and testing of a model.Take a look at the notebook Appendix_3_kfold_cross_validation if you'd like to try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
